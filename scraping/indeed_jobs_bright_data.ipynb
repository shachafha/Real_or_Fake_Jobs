{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f0f0a5b-2f64-42f0-9986-fb6a4f6aa365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install playwright\n",
    "!pip install nest_asyncio\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427ec553-4c30-4115-89bc-7c74a1d0a6fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba6bcc4-c526-4ef8-9685-c546146b00d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d067e8f-4a16-43bc-a844-c8234e18cd60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total job URLs: 3806\nJob URLs to scrape: 3492\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "\n",
    "from playwright.async_api import async_playwright\n",
    "import pandas as pd\n",
    "\n",
    "NAME = '<NAME>'\n",
    "WS = {'nitzan' : ['<web_socket_here>'],\n",
    "        'shani' : ['<web_socket_here>'],\n",
    "        'saar' : ['<web_socket_here>'],\n",
    "        'shachaf': ['<web_socket_here']}\n",
    "\n",
    "\n",
    "\n",
    "async def get_record(page):\n",
    "    '''Extract job data from a single job page using Playwright'''\n",
    "\n",
    "    # Initialize dictionary to store job data\n",
    "    job_data = {}\n",
    "    await page.mouse.move(random.randint(0, 500), random.randint(0, 500))\n",
    "    await page.wait_for_timeout(random.uniform(1000, 3000))\n",
    "\n",
    "    try:\n",
    "        # Extract job title\n",
    "        title_elem = await page.query_selector('h2[data-testid=\"simpler-jobTitle\"]')\n",
    "        job_data['title'] = await title_elem.inner_text()\n",
    "\n",
    "    except Exception as e:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Extract company name\n",
    "        company_elem = await page.query_selector('a.jobsearch-JobInfoHeader-companyNameLink')\n",
    "        job_data['company'] = await company_elem.inner_text()\n",
    "\n",
    "    except Exception as e:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Extract location\n",
    "        location_elem = await page.query_selector('div[data-testid=\"jobsearch-JobInfoHeader-companyLocation\"]')\n",
    "        job_data['location'] = await location_elem.inner_text()\n",
    "\n",
    "    except Exception as e:\n",
    "        return\n",
    "\n",
    "    # Extract pay, job type, work setting based on titles\n",
    "    try:\n",
    "        insights_elems = await page.query_selector_all('div.js-match-insights-provider-16m282m.e37uo190')\n",
    "        for insight in insights_elems:\n",
    "            title_elem = await insight.query_selector('h3.js-match-insights-provider-11n8e9a')\n",
    "            if title_elem:\n",
    "                title_text = await title_elem.inner_text()\n",
    "\n",
    "                # Extract data based on the title\n",
    "                if \"Pay\" in title_text:\n",
    "                    salary_elems = await insight.query_selector_all('span.js-match-insights-provider-4pmm6z')\n",
    "                    if salary_elems:\n",
    "                        job_data['salary'] = await salary_elems[0].inner_text() if salary_elems else \"N/A\"\n",
    "                elif \"Job type\" in title_text:\n",
    "                    # Extract all job types if multiple\n",
    "                    job_type_elems = await insight.query_selector_all(\n",
    "                        'ul.js-match-insights-provider-h884c4 li span.js-match-insights-provider-4pmm6z')\n",
    "                    if job_type_elems:\n",
    "                        job_data['job_type'] = ', '.join(\n",
    "                            [await job_type.inner_text() for job_type in job_type_elems]) if job_type_elems else \"N/A\"\n",
    "                elif \"Work setting\" in title_text:\n",
    "                    work_setting_elems = await insight.query_selector_all('span.js-match-insights-provider-4pmm6z')\n",
    "                    if work_setting_elems:\n",
    "                        job_data['work_setting'] = await work_setting_elems[\n",
    "                            0].inner_text() if work_setting_elems else \"N/A\" #\n",
    "                elif \"Shift and schedule\" in title_text:\n",
    "                    shift_and_schedule_elems = await insight.query_selector_all('span.js-match-insights-provider-4pmm6z')\n",
    "                    if shift_and_schedule_elems:\n",
    "                        job_data['shift_and_schedule'] = await shift_and_schedule_elems[\n",
    "                            0].inner_text() if shift_and_schedule_elems else \"N/A\" #\n",
    "                elif \"Medical specialty\" in title_text:\n",
    "                    medical_specialty_elems = await insight.query_selector_all('span.js-match-insights-provider-4pmm6z')\n",
    "                    if medical_specialty_elems:\n",
    "                        job_data['medical_specialty'] = await medical_specialty_elems[\n",
    "                            0].inner_text() if medical_specialty_elems else \"N/A\"\n",
    "                else:\n",
    "                    # Extract any other information under different titles\n",
    "                    other_info_elems = await insight.query_selector_all('span.js-match-insights-provider-4pmm6z')\n",
    "                    if other_info_elems:\n",
    "                        job_data['other_info'] = await other_info_elems[\n",
    "                            0].inner_text() if other_info_elems else \"N/A\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Extract job description\n",
    "        job_description_elem = await page.query_selector('div.jobsearch-JobComponent-description')\n",
    "        job_data['job_description'] = await job_description_elem.inner_text()\n",
    "\n",
    "    except Exception as e:\n",
    "        return\n",
    "\n",
    "    return job_data\n",
    "\n",
    "job_lock = asyncio.Lock()\n",
    "\n",
    "# Assuming you have a list of job URLs\n",
    "all_jobs = []\n",
    "\n",
    "# Load existing results if the file exists\n",
    "output_file = f'jobs_data_{NAME}.xlsx'\n",
    "if os.path.exists(output_file):\n",
    "    existing_data = pd.read_excel(output_file)\n",
    "    all_jobs = existing_data.to_dict(orient='records')\n",
    "else:\n",
    "    existing_data = pd.DataFrame()\n",
    "\n",
    "already_scraped = existing_data['job_url'].tolist() if not existing_data.empty else []\n",
    "# get urls from file job_links.wlsx in the column Job URL\n",
    "job_urls = pd.read_excel(f'job_links_{NAME}.xlsx')['Job URL'].tolist()\n",
    "job_urls = list(set([link for link in job_urls if 'pagead' not in link]))\n",
    "print(\"Total job URLs:\", len(job_urls))\n",
    "job_urls = [url for url in job_urls if url not in already_scraped]\n",
    "print(\"Job URLs to scrape:\", len(job_urls))\n",
    "\n",
    "\n",
    "async def handle_job(p, idx, url):\n",
    "    try:\n",
    "        browser = await p.chromium.connect_over_cdp(WS[NAME][idx % len(WS[NAME])])\n",
    "        # open browser without proxy\n",
    "        #browser = await p.chromium.launch()\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        await page.goto(url)\n",
    "        await page.mouse.move(random.randint(0, 500), random.randint(0, 500))\n",
    "        await page.wait_for_timeout(random.uniform(5000, 20000))\n",
    "        \n",
    "        job_data = await get_record(page) # Scrape data from the job page\n",
    "        if job_data:\n",
    "            job_data['job_url'] = url  # Add the URL to the data dictionary\n",
    "            async with job_lock:\n",
    "                all_jobs.append(job_data)  # Store the job data\n",
    "                new_data = pd.DataFrame(all_jobs)\n",
    "                \n",
    "                # Combine with existing data, ensuring no duplicates\n",
    "                combined_data = pd.concat([existing_data, new_data]).drop_duplicates(subset=['job_url'], keep='last')\n",
    "\n",
    "                # Save to Excel\n",
    "                combined_data.to_excel(output_file, index=False)\n",
    "\n",
    "            print(f\"Scraped job {idx+1}/{len(job_urls)}\")\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping job {idx+1}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "async def main():\n",
    "    ws_count = len(WS[NAME])\n",
    "    print(f\"Running {ws_count} workers\")\n",
    "    async with async_playwright() as p:\n",
    "        for i in range(0, len(job_urls), ws_count):\n",
    "            await asyncio.gather(*[handle_job(p, i + j, job_url) for j, job_url in enumerate(job_urls[i:i + ws_count])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc1b6f88-e408-422f-aac8-0c9a52c83d7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "asyncio.get_event_loop().run_until_complete(main())\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "jobs (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
