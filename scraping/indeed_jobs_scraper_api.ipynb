{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "116058e8-ec6f-4f27-9675-81ce2898be29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0b4fd8-daca-4a57-ac20-dbf56b51f9a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "NAME = '<Your_Name_Here>'\n",
    "API_KEY = '<Your_SCRAPER_API_KEY_Here>'\n",
    "SCRAPER_API_URL = 'https://api.scraperapi.com/'\n",
    "\n",
    "# Define headers for HTTP requests\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_job_data(job_url):\n",
    "    \"\"\"Extract job data from a job URL using ScraperAPI\"\"\"\n",
    "    try:\n",
    "        payload = {\n",
    "            'api_key': API_KEY,\n",
    "            'url': job_url\n",
    "        }\n",
    "        response = requests.get(SCRAPER_API_URL, params=payload, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the response text (HTML) and extract job data\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        job_data = {}\n",
    "\n",
    "        # Extract job title\n",
    "        title_elem = soup.select_one('h2[data-testid=\"simpler-jobTitle\"]')\n",
    "        job_data['title'] = title_elem.text.strip() if title_elem else 'N/A'\n",
    "\n",
    "        # Extract company name\n",
    "        company_elem = soup.select_one('a.jobsearch-JobInfoHeader-companyNameLink')\n",
    "        job_data['company'] = company_elem.text.strip() if company_elem else 'N/A'\n",
    "\n",
    "        # Extract location\n",
    "        location_elem = soup.select_one('div[data-testid=\"jobsearch-JobInfoHeader-companyLocation\"]')\n",
    "        job_data['location'] = location_elem.text.strip() if location_elem else 'N/A'\n",
    "\n",
    "        # Extract job description\n",
    "        description_elem = soup.select_one('div.jobsearch-JobComponent-description')\n",
    "        job_data['job_description'] = description_elem.text.strip() if description_elem else 'N/A'\n",
    "\n",
    "        insights_elems = soup.select('div.js-match-insights-provider-16m282m.e37uo190')\n",
    "        for insight in insights_elems:\n",
    "            # Extract the title\n",
    "            title_elem = insight.select_one('h3.js-match-insights-provider-11n8e9a')\n",
    "            title_text = title_elem.get_text(strip=True) if title_elem else \"\"\n",
    "\n",
    "            # Extract data based on the title\n",
    "            if \"Pay\" in title_text:\n",
    "                salary_elem = insight.select_one('span.js-match-insights-provider-4pmm6z')\n",
    "                job_data['salary'] = salary_elem.get_text(strip=True) if salary_elem else \"N/A\"\n",
    "            elif \"Job type\" in title_text:\n",
    "                job_type_elems = insight.select('ul.js-match-insights-provider-h884c4 li span.js-match-insights-provider-4pmm6z')\n",
    "                job_data['job_type'] = ', '.join(elem.get_text(strip=True) for elem in job_type_elems) if job_type_elems else \"N/A\"\n",
    "            elif \"Work setting\" in title_text:\n",
    "                work_setting_elem = insight.select_one('span.js-match-insights-provider-4pmm6z')\n",
    "                job_data['work_setting'] = work_setting_elem.get_text(strip=True) if work_setting_elem else \"N/A\"\n",
    "            elif \"Shift and schedule\" in title_text:\n",
    "                shift_schedule_elem = insight.select_one('span.js-match-insights-provider-4pmm6z')\n",
    "                job_data['shift_and_schedule'] = shift_schedule_elem.get_text(strip=True) if shift_schedule_elem else \"N/A\"\n",
    "            elif \"Medical specialty\" in title_text:\n",
    "                medical_specialty_elem = insight.select_one('span.js-match-insights-provider-4pmm6z')\n",
    "                job_data['medical_specialty'] = medical_specialty_elem.get_text(strip=True) if medical_specialty_elem else \"N/A\"\n",
    "            else:\n",
    "                # Extract other information under different titles\n",
    "                other_info_elem = insight.select_one('span.js-match-insights-provider-4pmm6z')\n",
    "                job_data['other_info'] = other_info_elem.get_text(strip=True) if other_info_elem else \"N/A\"\n",
    "\n",
    "\n",
    "        return job_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {job_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Load job URLs from file\n",
    "    job_links_file = f'job_links_{NAME}.xlsx'\n",
    "    output_file = f'jobs_data_{NAME}_scraperapi.xlsx'\n",
    "\n",
    "    if not os.path.exists(job_links_file):\n",
    "        print(f\"Error: {job_links_file} not found.\")\n",
    "        return\n",
    "\n",
    "    job_urls = pd.read_excel(job_links_file)['Job URL'].tolist()\n",
    "    job_urls = list(set([link for link in job_urls if 'pagead' not in link]))\n",
    "\n",
    "    # Load existing data if available\n",
    "    all_jobs = []\n",
    "    if os.path.exists(output_file):\n",
    "        existing_data = pd.read_excel(output_file)\n",
    "        all_jobs = existing_data.to_dict(orient='records')\n",
    "        already_scraped = existing_data['job_url'].tolist()\n",
    "    else:\n",
    "        already_scraped = []\n",
    "\n",
    "    existing_data2 = pd.read_excel(f'jobs_data_{NAME}.xlsx')\n",
    "    already_scraped2 = existing_data2['job_url'].tolist()\n",
    "\n",
    "    job_urls = [url for url in job_urls if url not in already_scraped and url not in already_scraped2]\n",
    "    print(\"Total job URLs to scrape:\", len(job_urls))\n",
    "\n",
    "    for i, job_url in enumerate(job_urls):\n",
    "        print(f\"Scraping job {i + 1}/{len(job_urls)}: {job_url}\")\n",
    "        job_data = get_job_data(job_url)\n",
    "        if job_data:\n",
    "            job_data['job_url'] = job_url\n",
    "            all_jobs.append(job_data)\n",
    "            new_data = pd.DataFrame(all_jobs)\n",
    "            new_data.to_excel(output_file, index=False)\n",
    "            print(f\"Saved {len(all_jobs)} records to {output_file}.\")\n",
    "        # Wait to avoid rate-limiting\n",
    "        time.sleep(random.uniform(1, 4))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "scraperAPI",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
